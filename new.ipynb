{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X variables:\n",
      "    Month  DayOfWeek  EvenCount  OddCount  HighCount  LowCount  \\\n",
      "0      5          3          3         3          1         5   \n",
      "1      5          0          4         2          4         2   \n",
      "2      5          3          4         2          4         2   \n",
      "3      5          0          2         4          2         4   \n",
      "4      5          3          0         6          4         2   \n",
      "\n",
      "   Winning Number 1  Winning Number 2  Winning Number 3  Winning Number 4  \\\n",
      "0                 5                 8                13                20   \n",
      "1                21                22                40                41   \n",
      "2                15                20                36                38   \n",
      "3                 5                12                15                21   \n",
      "4                 7                13                35                39   \n",
      "\n",
      "   Winning Number 5  Winning Number 6  \n",
      "0                21                46  \n",
      "1                42                46  \n",
      "2                41                46  \n",
      "3                30                39  \n",
      "4                41                45  \n",
      "\n",
      "y variable:\n",
      " 0    1\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "Name: WinningGroup, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ============================\n",
    "# ğŸ”¹ 1ï¸âƒ£ LOAD & PREPROCESS DATA ğŸ”¹\n",
    "# ============================\n",
    "\n",
    "# âœ… Step 1: Load Dataset\n",
    "df = pd.read_csv('Toto Winning Numbers.csv')\n",
    "\n",
    "# âœ… Step 2: Preprocess Data\n",
    "df.columns = df.columns.str.strip()  # Clean column names\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "\n",
    "# Extract Winning Number columns\n",
    "winning_cols = [col for col in df.columns if 'Winning Number' in col]\n",
    "\n",
    "# Step 3: Feature Engineering\n",
    "df['EvenCount'] = df[winning_cols].apply(lambda x: sum(num % 2 == 0 for num in x), axis=1)\n",
    "df['OddCount'] = df[winning_cols].apply(lambda x: sum(num % 2 != 0 for num in x), axis=1)\n",
    "df['HighCount'] = df[winning_cols].apply(lambda x: sum(num > 25 for num in x), axis=1)\n",
    "df['LowCount'] = df[winning_cols].apply(lambda x: sum(num <= 25 for num in x), axis=1)\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"Extract statistical features from the lottery results for model training.\"\"\"\n",
    "    \n",
    "    winning_cols = [col for col in df.columns if 'Winning Number' in col]\n",
    "    all_numbers = np.arange(1, 50)\n",
    "\n",
    "    # Initialize feature dictionary\n",
    "    feature_dict = {\n",
    "        num: {'Hot': 0, 'Cold': 1, 'Even': 0, 'Odd': 0, 'High': 0, 'Low': 0, 'Repeat': 0}\n",
    "        for num in all_numbers\n",
    "    }\n",
    "\n",
    "    past_draws = []  # Store previous draw results for repeat number tracking\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        drawn_numbers = row[winning_cols].values  \n",
    "        \n",
    "        for num in all_numbers:\n",
    "            feature_dict[num]['Hot'] += 1 if num in drawn_numbers else 0  \n",
    "            feature_dict[num]['Cold'] = 0 if num in drawn_numbers else feature_dict[num]['Cold']  \n",
    "            feature_dict[num]['Even'] = 1 if num % 2 == 0 else 0  \n",
    "            feature_dict[num]['Odd'] = 1 if num % 2 != 0 else 0\n",
    "            feature_dict[num]['High'] = 1 if num > 25 else 0  \n",
    "            feature_dict[num]['Low'] = 1 if num <= 25 else 0\n",
    "            feature_dict[num]['Repeat'] = 1 if num in past_draws else 0  \n",
    "\n",
    "        past_draws = drawn_numbers  \n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    features = pd.DataFrame.from_dict(feature_dict, orient='index').reset_index()\n",
    "    features.rename(columns={'index': 'Number'}, inplace=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Apply feature engineering\n",
    "df, features = add_features(df)\n",
    "\n",
    "# âœ… Step 4: Define Target Variable (Winning Group)\n",
    "df['WinningGroup'] = (df['Division 1 Winners'] > 0).astype(int)\n",
    "\n",
    "# âœ… Step 5: Select Features & Split Data\n",
    "X = df[['Month', 'DayOfWeek', 'EvenCount', 'OddCount', 'HighCount', 'LowCount'] + winning_cols]\n",
    "y = df['WinningGroup']\n",
    "\n",
    "print(\"X variables:\\n\", X.head())\n",
    "print(\"\\ny variable:\\n\", y.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ===================================\n",
    "# ğŸ”¹ 5ï¸âƒ£ MACHINE LEARNING MODELS ğŸ”¹\n",
    "# ===================================\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=15, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  \n",
    "    predictions = model.predict(X_test)  \n",
    "\n",
    "    print(f\"\\nğŸ“Š {name} Performance for Winning Group Prediction:\")\n",
    "    print(classification_report(y_test, predictions))  \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # ğŸ” Interpretation\n",
    "    if name == \"Random Forest\":\n",
    "        print(\"ğŸ”¹ The Random Forest model generally performs well in handling complex patterns.\\n\"\n",
    "              \"ğŸ”¹ If accuracy is high (>60%), this suggests that month/day trends have some predictive power.\\n\"\n",
    "              \"ğŸ”¹ If accuracy is low (~50%), it means winning draws are more random and harder to predict.\\n\")\n",
    "        \n",
    "    elif name == \"Decision Tree\":\n",
    "        print(\"ğŸ”¹ Decision Trees help visualize how winning numbers correlate with the month/day.\\n\"\n",
    "              \"ğŸ”¹ If accuracy is close to Random Forest, it suggests strong feature importance.\\n\"\n",
    "              \"ğŸ”¹ If accuracy is lower than Random Forest, it means overfitting or weak patterns.\\n\")\n",
    "\n",
    "    elif name == \"KNN\":\n",
    "        print(\"ğŸ”¹ KNN predicts based on historical proximity.\\n\"\n",
    "              \"ğŸ”¹ If accuracy is poor (~50%), it suggests past results have little impact on future draws.\\n\")\n",
    "\n",
    "# ====================================================\n",
    "# ğŸ”¹ 6ï¸âƒ£ FEATURE IMPORTANCE (TREE-BASED MODELS) ğŸ”¹\n",
    "# ====================================================\n",
    "\n",
    "rf_model = models['Random Forest']\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances, y=feature_names, palette=\"viridis\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Œ **Feature Importance Interpretation:**\\n\"\n",
    "      \"ğŸ”¹ If 'Month' is most important, then some months may be more likely to have winners.\\n\"\n",
    "      \"ğŸ”¹ If 'DayOfWeek' is dominant, jackpot draws may favor certain days.\\n\")\n",
    "\n",
    "# ============================\n",
    "# ğŸ”¹ 7ï¸âƒ£ WINNING NUMBER ANALYSIS ğŸ”¹\n",
    "# ============================\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "winning_numbers = df[[col for col in df.columns if 'Winning Number' in col]].values.flatten()\n",
    "sns.histplot(winning_numbers, bins=np.arange(1, 51) - 0.5, kde=False, color=\"blue\")\n",
    "plt.xticks(np.arange(1, 50))\n",
    "plt.xlabel(\"Winning Numbers\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Winning Number Frequency Analysis\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Œ **Winning Number Frequency Analysis:**\\n\"\n",
    "      \"ğŸ”¹ Taller bars indicate numbers that appear more frequently.\\n\"\n",
    "      \"ğŸ”¹ If all numbers have similar frequency, then draws are well-randomized.\\n\")\n",
    "\n",
    "# ============================\n",
    "# ğŸ”¹ 8ï¸âƒ£ ODD/EVEN & HIGH/LOW ANALYSIS ğŸ”¹\n",
    "# ============================\n",
    "\n",
    "# âœ… Ensure winning numbers are properly extracted\n",
    "winning_numbers = df[[col for col in df.columns if 'Winning Number' in col]].values.flatten()\n",
    "winning_numbers = winning_numbers[~np.isnan(winning_numbers)]\n",
    "\n",
    "# ğŸ“Š Odd vs Even Distribution\n",
    "odd_even_counts = [np.sum(winning_numbers % 2 == 1), np.sum(winning_numbers % 2 == 0)]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(odd_even_counts, labels=['Odd', 'Even'], autopct='%1.1f%%', \n",
    "        colors=['red', 'blue'], startangle=140, wedgeprops={'edgecolor': 'black'})\n",
    "plt.title(\"Odd vs Even Number Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Œ **Odd vs Even Distribution:**\\n\"\n",
    "      f\"ğŸ”¹ Odd numbers appeared {odd_even_counts[0]} times.\\n\"\n",
    "      f\"ğŸ”¹ Even numbers appeared {odd_even_counts[1]} times.\\n\"\n",
    "      \"ğŸ”¹ If odd/even numbers are balanced (~50%), then the draw is likely unbiased.\\n\"\n",
    "      \"ğŸ”¹ If one dominates (>60%), it may indicate a trend.\\n\")\n",
    "\n",
    "# ğŸ“Š High vs Low Number Distribution (Using actual counts)\n",
    "high_low_counts = [np.sum(winning_numbers > 25), np.sum(winning_numbers <= 25)]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(high_low_counts, labels=['High (26-49)', 'Low (1-25)'], autopct='%1.1f%%', \n",
    "        colors=['green', 'yellow'], startangle=90, wedgeprops={'edgecolor': 'black'})\n",
    "plt.title(\"High vs Low Number Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Œ **High vs Low Number Analysis:**\\n\"\n",
    "      f\"ğŸ”¹ High numbers (26-49) appeared {high_low_counts[0]} times.\\n\"\n",
    "      f\"ğŸ”¹ Low numbers (1-25) appeared {high_low_counts[1]} times.\\n\"\n",
    "      \"ğŸ”¹ If high and low numbers are balanced (~50%), then the draw is likely random.\\n\"\n",
    "      \"ğŸ”¹ If one category dominates (>60%), it suggests a potential trend in high/low number selection.\\n\")\n",
    "\n",
    "# ============================\n",
    "# ğŸ”¹ 9ï¸âƒ£ NUMBER RECOMMENDATION ğŸ”¹\n",
    "# ============================\n",
    "\n",
    "def suggest_numbers(features, n=6):\n",
    "    hot_numbers = features.sort_values(by='Hot', ascending=False).head(n//2)['Number'].tolist()\n",
    "    cold_numbers = features.sort_values(by='Cold', ascending=False).head(n//2)['Number'].tolist()\n",
    "    recommended_numbers = list(set(hot_numbers + cold_numbers))[:n]  \n",
    "\n",
    "    return {\"Hot\": hot_numbers, \"Cold\": cold_numbers, \"Balanced Selection\": recommended_numbers}\n",
    "\n",
    "number_suggestions = suggest_numbers(features)\n",
    "\n",
    "hot_cold_df = features.sort_values(by='Hot', ascending=False).head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Number', y='Hot', data=hot_cold_df, color='red', label='Hot Numbers')\n",
    "sns.barplot(x='Number', y='Cold', data=hot_cold_df, color='blue', label='Cold Numbers')\n",
    "plt.title(\"Top 10 Hot vs Cold Numbers\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ”¥ **Hot Picks (Most Frequent Numbers):** {number_suggestions['Hot']}\")\n",
    "print(f\"â„ï¸ **Cold Picks (Least Drawn Numbers):** {number_suggestions['Cold']}\")\n",
    "print(f\"ğŸ¯ **Balanced Selection:** {number_suggestions['Balanced Selection']}\")\n",
    "\n",
    "print(\"ğŸ“Œ **Number Recommendation Interpretation:**\\n\"\n",
    "      \"ğŸ”¹ Hot numbers are historically frequent, but past results don't guarantee future wins.\\n\"\n",
    "      \"ğŸ”¹ Cold numbers are rarely drawn, but may be overdue for selection.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
